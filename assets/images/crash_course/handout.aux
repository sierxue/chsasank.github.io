\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{Calculus: Recap}{2}{section*.1}}
\newlabel{calculus-recap}{{}{2}{Calculus: Recap}{section*.1}{}}
\@writefile{toc}{\contentsline {subsection}{Derivative}{2}{subsection*.2}}
\newlabel{derivative}{{}{2}{Derivative}{subsection*.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Derivative illustration. Red is for positive \(v\) direction and green is for negative \(v\) direction. \href  {https://en.wikipedia.org/wiki/Derivative}{Source}.}}{2}{figure.1}}
\@writefile{toc}{\contentsline {subsection}{Gradient}{2}{subsection*.3}}
\newlabel{gradient}{{}{2}{Gradient}{subsection*.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Gradient of the 2D function \(f(x, y) = xe^{\GenericError  {(inputenc)                }{Package inputenc Error: Unicode character âˆ’ (U+2212)\MessageBreak not set up for use with LaTeX}{See the inputenc package documentation for explanation.}{You may provide a definition with\MessageBreak \GenericError  {               }{LaTeX Error: Can be used only in preamble}{See the LaTeX manual or LaTeX Companion for explanation.}{Your command was ignored.\MessageBreak Type  I <command> <return>  to replace it with another command,\MessageBreak or  <return>  to continue without it.}}(x^2 + y^2)}\) is plotted as blue arrows over the pseudocolor (red is for high values while blue is for low values) plot of the function. \href  {https://en.wikipedia.org/wiki/Gradient}{Source}.}}{2}{figure.2}}
\@writefile{toc}{\contentsline {section}{Optimization}{2}{section*.4}}
\newlabel{optimization}{{}{2}{Optimization}{section*.4}{}}
\@writefile{toc}{\contentsline {subsection}{Gradient Descent}{2}{subsection*.5}}
\newlabel{gradient-descent}{{}{2}{Gradient Descent}{subsection*.5}{}}
\newpmemlabel{^_1}{2}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Gradient descent illustration: a ball on a valley. \href  {http://neuralnetworksanddeeplearning.com/chap1.html}{Source}.}}{2}{figure.3}}
\newlabel{fig:valley}{{3}{2}{Gradient Descent}{figure.3}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Gradient Descent}}{3}{algorithm.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Gradient descent on a series of level sets. \href  {https://en.wikipedia.org/wiki/Gradient_descent}{Source}.}}{3}{figure.4}}
\@writefile{toc}{\contentsline {subsection}{Example: Regression}{3}{subsection*.6}}
\newlabel{example-regression}{{}{3}{Example: Regression}{subsection*.6}{}}
\@writefile{toc}{\contentsline {subsection}{Stochastic Gradient Descent}{3}{subsection*.7}}
\newlabel{stochastic-gradient-descent}{{}{3}{Stochastic Gradient Descent}{subsection*.7}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Stochastic Gradient Descent}}{4}{algorithm.2}}
\@writefile{toc}{\contentsline {section}{Neural Networks}{4}{section*.8}}
\newlabel{neural-networks}{{}{4}{Neural Networks}{section*.8}{}}
\@writefile{toc}{\contentsline {subsection}{Perceptron}{4}{subsection*.9}}
\newlabel{perceptron}{{}{4}{Perceptron}{subsection*.9}{}}
\newpmemlabel{^_2}{4}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  Perceptron Model. \href  {http://neuralnetworksanddeeplearning.com/chap1.html}{Source}. }}{4}{figure.5}}
\newlabel{fig:percp}{{5}{4}{Perceptron}{figure.5}{}}
\newpmemlabel{^_3}{5}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \texttt  {NAND} implemented by perceptron. \href  {http://neuralnetworksanddeeplearning.com/chap1.html}{Source} }}{5}{figure.6}}
\newlabel{fig:nand}{{6}{5}{Perceptron}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{Multi Layer Perceptrons and Sigmoid}{6}{subsection*.10}}
\newlabel{multi-layer-perceptrons-and-sigmoid}{{}{6}{Multi Layer Perceptrons and Sigmoid}{subsection*.10}{}}
\newpmemlabel{^_4}{6}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Multi layer perceptron. \href  {http://neuralnetworksanddeeplearning.com/chap1.html}{Source} }}{6}{figure.7}}
\newlabel{fig:mlp}{{7}{6}{Multi Layer Perceptrons and Sigmoid}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Sigmoid function. When \(z\) is large and positive, Then \(e^{-z} \approx 0\) and so \(\sigma (z) \approx 1\). Suppose on the other hand that \(z\) is very negative. Then \(e^{-z} \rightarrow \infty \), and \(\sigma (z) \approx 0\). \href  {http://neuralnetworksanddeeplearning.com/chap1.html}{Source}.}}{7}{figure.8}}
\@writefile{toc}{\contentsline {subsection}{ReLU Activation}{7}{subsection*.11}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces  ReLU. \href  {http://neuralnetworksanddeeplearning.com/chap3.html}{Source}. }}{7}{figure.9}}
\@writefile{toc}{\contentsline {subsection}{Loss functions}{7}{subsection*.12}}
\@writefile{toc}{\contentsline {subsection}{Cross Entropy Loss}{8}{subsection*.13}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces  MLP for digit classification. \href  {http://neuralnetworksanddeeplearning.com/chap1.html\%22}{Source}. }}{8}{figure.10}}
\newlabel{fig:mlp-digit}{{10}{8}{Cross Entropy Loss}{figure.10}{}}
\@writefile{toc}{\contentsline {subsection}{Softmax Activation}{8}{subsection*.14}}
\@writefile{toc}{\contentsline {subsection}{Backpropogation}{8}{subsection*.15}}
\newlabel{backpropogation}{{}{8}{Backpropogation}{subsection*.15}{}}
\newpmemlabel{^_5}{9}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Cost function as a composition of functions}}{9}{figure.11}}
\newlabel{eq:1}{{1}{9}{Backpropogation}{equation.0.1}{}}
\newlabel{eq:2}{{2}{9}{Backpropogation}{equation.0.2}{}}
\newlabel{eq:3}{{3}{9}{Backpropogation}{equation.0.3}{}}
\newlabel{eq:4}{{4}{10}{Backpropogation}{equation.0.4}{}}
\newlabel{eq:5}{{5}{10}{Backpropogation}{equation.0.5}{}}
\newlabel{eq:6}{{6}{10}{Backpropogation}{equation.0.6}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Back Propogation}}{11}{algorithm.3}}
\@writefile{toc}{\contentsline {subsection}{Deep Networks and why they are hard to train}{11}{subsection*.16}}
\newlabel{deep-neural-networks}{{}{11}{Deep Networks and why they are hard to train}{subsection*.16}{}}
\newpmemlabel{^_6}{11}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Logical circuit for multiplication. \href  {http://neuralnetworksanddeeplearning.com/chap5.html\%22}{Source}. }}{11}{figure.12}}
\newlabel{fig:circuitmult}{{12}{11}{Deep Networks and why they are hard to train}{figure.12}{}}
\newpmemlabel{^_7}{11}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Deep Neural Network. \href  {http://neuralnetworksanddeeplearning.com/chap5.html\%22}{Source}.}}{12}{figure.13}}
\newlabel{fig:dnn}{{13}{12}{Deep Networks and why they are hard to train}{figure.13}{}}
\@writefile{toc}{\contentsline {section}{Convolutional Neural Networks}{12}{section*.17}}
\newlabel{convolutional-neural-networks}{{}{12}{Convolutional Neural Networks}{section*.17}{}}
\newpmemlabel{^_8}{12}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces MLP for MNIST. \href  {http://neuralnetworksanddeeplearning.com/chap5.html\%22}{Source}.}}{12}{figure.14}}
\newlabel{fig:mnistmlp}{{14}{12}{Convolutional Neural Networks}{figure.14}{}}
\@writefile{toc}{\contentsline {subsection}{Local receptive fileds}{13}{subsection*.18}}
\newpmemlabel{^_9}{13}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Local receptive fields of convolution. \href  {http://neuralnetworksanddeeplearning.com/chap6.html\%22}{Source}. }}{13}{figure.15}}
\newpmemlabel{^_10}{14}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Convolution. \href  {http://neuralnetworksanddeeplearning.com/chap6.html\%22}{Source}. }}{14}{figure.16}}
\newlabel{fig:conv2s}{{16}{14}{Local receptive fileds}{figure.16}{}}
\newpmemlabel{^_11}{14}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Slide the convolution. \href  {http://neuralnetworksanddeeplearning.com/chap6.html\%22}{Source}. }}{14}{figure.17}}
\newlabel{fig:conv2sd}{{17}{14}{Local receptive fileds}{figure.17}{}}
\@writefile{toc}{\contentsline {subsection}{Shared weights and biases}{14}{subsection*.19}}
\newpmemlabel{^_12}{14}
\newpmemlabel{^_13}{14}
\newpmemlabel{^_14}{14}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces A convolutional filter. \href  {https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/}{Source}. }}{15}{figure.18}}
\newlabel{fig:1}{{18}{15}{Shared weights and biases}{figure.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Convolution on an example image. \href  {https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/}{Source}. }}{15}{figure.19}}
\newlabel{fig:conv2}{{19}{15}{Shared weights and biases}{figure.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Apply convolution. \href  {https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/}{Source}.  }}{15}{figure.20}}
\newlabel{fig:conv3}{{20}{15}{Shared weights and biases}{figure.20}{}}
\newpmemlabel{^_15}{15}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Apply convolution at a different receptive field. \href  {https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/}{Source}. }}{15}{figure.21}}
\newlabel{fig:conv4}{{21}{15}{Shared weights and biases}{figure.21}{}}
\newpmemlabel{^_16}{16}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Apply convolution at a different receptive field. \href  {http://neuralnetworksanddeeplearning.com/chap6.html}{Source}. }}{16}{figure.22}}
\@writefile{toc}{\contentsline {subsection}{Pooling layers}{16}{subsection*.20}}
\newpmemlabel{^_17}{16}
\newpmemlabel{^_18}{16}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Pooling Layer. \href  {http://neuralnetworksanddeeplearning.com/chap6.html}{Source}. }}{17}{figure.23}}
\newlabel{fig:pool}{{23}{17}{Pooling layers}{figure.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Convolution and pooling layer. \href  {http://neuralnetworksanddeeplearning.com/chap6.html}{Source}. }}{17}{figure.24}}
\newlabel{fig:pool2}{{24}{17}{Pooling layers}{figure.24}{}}
\@writefile{toc}{\contentsline {subsection}{Case Study: LeNet}{17}{subsection*.21}}
\newlabel{case-study-lenet}{{}{17}{Case Study: LeNet}{subsection*.21}{}}
\newpmemlabel{^_19}{17}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces LeNet architecture. \href  {http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf}{Source}. }}{17}{figure.25}}
\@writefile{toc}{\contentsline {section}{Tricks of the Trade}{18}{section*.22}}
\newlabel{tricks-of-the-trade}{{}{18}{Tricks of the Trade}{section*.22}{}}
\@writefile{toc}{\contentsline {subsection}{Dropout}{18}{subsection*.23}}
\newlabel{dropout}{{}{18}{Dropout}{subsection*.23}{}}
\newpmemlabel{^_20}{18}
\newpmemlabel{^_21}{18}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Network before dropout}}{19}{figure.26}}
\newlabel{fig:dropout1}{{26}{19}{Dropout}{figure.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces Network after dropout}}{19}{figure.27}}
\newlabel{fig:dropout2}{{27}{19}{Dropout}{figure.27}{}}
\@writefile{toc}{\contentsline {subsection}{Data Augmentation}{19}{subsection*.24}}
\newlabel{data-augmentation}{{}{19}{Data Augmentation}{subsection*.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces Example tranining image}}{20}{figure.28}}
\newlabel{fig:mnist1}{{28}{20}{Data Augmentation}{figure.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces Rotated training image}}{20}{figure.29}}
\newlabel{fig:mnist2}{{29}{20}{Data Augmentation}{figure.29}{}}
\@writefile{toc}{\contentsline {subsection}{Weight initialization and Batch Normalization}{20}{subsection*.25}}
\newlabel{weight-initialization-and-batch-normalization}{{}{20}{Weight initialization and Batch Normalization}{subsection*.25}{}}
\@writefile{toc}{\contentsline {section}{Practical Advice}{21}{section*.26}}
\newlabel{practical-advice}{{}{21}{Practical Advice}{section*.26}{}}
\@writefile{toc}{\contentsline {subsection}{ImageNet Dataset and ILSVRC}{21}{subsection*.27}}
\newlabel{imagenet-dataset-and-ilsvrc}{{}{21}{ImageNet Dataset and ILSVRC}{subsection*.27}{}}
\newpmemlabel{^_22}{21}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces Images from imagenet. \href  {http://mappingignorance.org/fx/media/2013/04/Deep-learning-5.png}{Source}. }}{21}{figure.30}}
\newlabel{fig:imagenet}{{30}{21}{ImageNet Dataset and ILSVRC}{figure.30}{}}
\newpmemlabel{^_23}{21}
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces Alexnet architecture. \href  {http://mappingignorance.org/fx/media/2013/04/Deep-learning-5.png}{Source}. }}{21}{figure.31}}
\newlabel{fig:alexnet}{{31}{21}{ImageNet Dataset and ILSVRC}{figure.31}{}}
\newpmemlabel{^_24}{22}
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces Top 1 accuracies on ILSVRC. \href  {https://chaosmail.github.io/deeplearning/2016/10/22/intro-to-deep-learning-for-computer-vision/\#Canziani16}{Source}. }}{22}{figure.32}}
\newlabel{fig:top1}{{32}{22}{ImageNet Dataset and ILSVRC}{figure.32}{}}
\@writefile{toc}{\contentsline {subsection}{Transfer Learning}{22}{subsection*.28}}
\newlabel{transfer-learning}{{}{22}{Transfer Learning}{subsection*.28}{}}
\newpmemlabel{^_25}{22}
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces Ants/Bees classifier. \href  {http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html}{Source}. }}{23}{figure.33}}
\@writefile{toc}{\contentsline {subsection}{GPUs}{23}{subsection*.29}}
\newlabel{gpus}{{}{23}{GPUs}{subsection*.29}{}}
\@writefile{toc}{\contentsline {subsection}{Other FAQ}{23}{subsection*.30}}
\newlabel{other-faq}{{}{23}{Other FAQ}{subsection*.30}{}}
\@writefile{toc}{\contentsline {section}{Recommended reading}{24}{section*.31}}
\ttl@finishall
